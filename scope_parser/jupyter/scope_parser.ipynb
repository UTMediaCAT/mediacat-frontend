{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T20:52:48.738170Z",
     "start_time": "2020-10-13T20:52:47.801647Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T20:52:48.743594Z",
     "start_time": "2020-10-13T20:52:48.740174Z"
    }
   },
   "outputs": [],
   "source": [
    "def insert_UniqueID(df):\n",
    "    \"\"\"\n",
    "    insert a new column of Unique ID from 1,2,3,4,5,...to n\n",
    "    :param df: a dataframe\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    df.insert(0, \"UniqueID\", range(1, len(df) + 1), True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T20:52:48.752357Z",
     "start_time": "2020-10-13T20:52:48.746423Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_Twitter_Handle(df):\n",
    "    \"\"\"\n",
    "    expand the records for all the associated twitter handle\n",
    "    :param df: a dataframe\n",
    "    :rtype: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # to find every non-empty Associate Twitter Handle's rows's index and\n",
    "    # store into not_null_twitter \n",
    "    # for example if row 0 exist associate twitter handle, then it will\n",
    "    # be stored in not_null_twitter \n",
    "    not_null_twitter = df.loc[pd.notna(df[\"Associated Twitter Handle\"]), :].index\n",
    "\n",
    "    # create new rows for every Associate Twitter Handle, in this case source\n",
    "    # will be the old assciated twitter handle\n",
    "    # Type to be \"Twitter Handle\", UniqueID, Tags and Name will keep same.\n",
    "    new_df_to_add_twitter = pd.DataFrame({'UniqueID': df.iloc[not_null_twitter, 0],  # nopep8\n",
    "                                  'Source': df.iloc[not_null_twitter, 5],  # nopep8\n",
    "                                  \"RSS feed URLs (where available)\": np.nan,  # nopep8\n",
    "                                  'Type': \"Twitter Handle\",  # nopep8\n",
    "                                  'Tags': df.iloc[not_null_twitter, 4],  # nopep8\n",
    "                                  'Associated Twitter Handle': np.nan,  # nopep8\n",
    "                                  'Associated Publisher': np.nan,  # nopep8\n",
    "                                  'Name': df.iloc[not_null_twitter, 7],  # nopep8        \n",
    "                                  'Text aliases': np.nan})\n",
    "\n",
    "    # since some of associated twitter Handle will be 2 or 3,\n",
    "    # so it use \"|\" to split\n",
    "    # and make every twitter handle as a new record\n",
    "    new_df_to_add_twitter[\"Source\"] = new_df_to_add_twitter[\"Source\"].str.split(\"|\")\n",
    "    new_df_to_add_twitter = new_df_to_add_twitter.explode(\"Source\")\n",
    "    return new_df_to_add_twitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T20:52:48.758506Z",
     "start_time": "2020-10-13T20:52:48.755052Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def separate_by_type(df):\n",
    "    \"\"\"\n",
    "    separate the dataframe that group by Type\n",
    "    :param df: a dataframe\n",
    "    :rtype: list of dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    Object = [x for _,x in df.groupby('Type')]\n",
    "    return Object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T20:52:48.764720Z",
     "start_time": "2020-10-13T20:52:48.760857Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_twitter_csv(file1):\n",
    "    \"\"\"\n",
    "    generate twitter.csv with unicode version\n",
    "    :param df: a dataframe\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    file1.to_csv('twitter.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "    \n",
    "def get_domain_csv(file0):\n",
    "    \"\"\"\n",
    "    generate domain.csv with unicode version\n",
    "    :param df: a dataframe\n",
    "    :rtype: None\n",
    "    \"\"\" \n",
    "    file0.to_csv('domain.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T20:52:48.773710Z",
     "start_time": "2020-10-13T20:52:48.767397Z"
    }
   },
   "outputs": [],
   "source": [
    "def scope_parser(file):\n",
    "    \"\"\"\n",
    "    do the whole scoper_parser's process\n",
    "    :param file: a csv file\n",
    "    :rtype: list of dataframe\n",
    "    \"\"\"\n",
    "    # read the csv file\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # create a new column named \"UniqueID\" from 1,2,3,4,5,\n",
    "    # ...to n(n is # of rows we have)\n",
    "    # UniqueID is using to link the Source and \n",
    "    # Associate Twitter Handle, which should create by system\n",
    "    # usually it is used by system only\n",
    "    insert_UniqueID(df)\n",
    "    \n",
    "    expand_records = expand_Twitter_Handle(df)\n",
    "    # add expand_records into dafaframe and reset\n",
    "    # the index to 0,1,2,3....\n",
    "    df = df.append(expand_records)\n",
    "    df = df.sort_values(by=['UniqueID'])\n",
    "    df = df.reset_index()\n",
    "    df.drop(['index'], axis=1, inplace=True)\n",
    "    \n",
    "    # separate dataframe that group by Type\n",
    "    Object = separate_by_type(df)\n",
    "    # generate twitter csv\n",
    "    print(\"generate twitter.csv\")\n",
    "    get_twitter_csv(Object[1])\n",
    "    # generate domain csv\n",
    "    print(\"generate domain.csv\")\n",
    "    get_domain_csv(Object[0])   \n",
    "    \n",
    "    print(\"Congratulations, you could move to next step!\")\n",
    "    # returen dataframe\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T20:52:48.811544Z",
     "start_time": "2020-10-13T20:52:48.776175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate twitter.csv\n",
      "generate domain.csv\n",
      "Congratulations, you could move to next step!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    inFile = sys.argv[1]\n",
    "    dataframes = scope_parser(\"test_demo.csv\")  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
